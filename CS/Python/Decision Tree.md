[출처 : Google research : Decision tree](https://colab.research.google.com/drive/11TllsCCDFzTbg7BBEQxi_5ujVo0_ODw4#scrollTo=76Me5LAPhUOi)

### 정의
---
의사결정 나무는 여러 가지 규칙을 순차적으로 적용하면서 독립 변수 공간을 분할하는 분류 모형이다. 분류(Classification)와 회귀(Regression)에 모두 사용될 수 있기 때문에 CART(Classification And Regression Tree)라고도 한다.
`독립 변수 : 통계와 머신러닝에서 사용될 때는 조작되는 변수로써 종속변수에 영향을 끼치는 변수를 뜻한다. 이 문장의 문맥에서는 독립변수들은 의사결정을 판단할 특징이나 속성을 말한다 by chatgpt`
`독립 변수 공간 : 이 문장에서는 알고리즘이 특정한 조건에 따라 예측하거나 분류하는 공간을 뜻한다`

`이해 : 독립 변수가 의사결정에 영향을 끼치는 것을 뜻하고 그런 변수를 통해 트리구조로 분할되는 것을 뜻하는 것 같음.`

`recursion vs regression : 헷갈렸다.. recursion은 재귀함수의 재귀로 보통 컴퓨터공학이나 수학에서 하나의 문제가 작은 단위로 쪼개질 수 있을 때, 함수 식 안에서 스스로의 함수를 사용하여 거듭해서 수행하는 것을 뜻하고 regression은 회귀로 종속변수와 독립변수간의 관계를 분석할 때 사용하는 용어이다. `

### 의사결정나무를 이용한 분류학습
---
의사결정나무를 이용한 분류법은 다음과 같다.
1. 여러가지 독립 변수 중 하나의 독립 변수를 선택하고 그 독립 변수에 대한 기준값(threshold)를 정한다. 이를 분류 규칙이라고 한다. 최적의 분류 규칙을 찾는 방법은 이후에 자세히 설명한다.
2. 전체 학습 뎅이터 집합(부모 노드)을 독립 변수의 값이 기준값보다 작은 데이터 그룹(자삭노드1)과 해당 독립 변수의 값이 기준값보다 큰 데이터 그룹(자식노드2)으로 나눈다.
3. 각각의 자식 노드에 대해 1~2의 단계를 반복하여 하위의 자식 노드를 만든다. 단, 자식 노드에 한가지 클래스의 데이터만 존재한다면 더 이상 자식 노드를 나누지 않고 중지한다.
이렇게 자식노드 나누기를 연속적으로 적용하면 노드가 계속 증가하는 나무(tree)와 같은 형태로 표현할 수 있다.
`1~2번만 읽어보면 완전 이진트리를 만들기 위한 로직처럼 보인다. 기준값(threshold)과 비교하여 분류를 계속해나가기 때문이다. 하지만 3번의 "단, 자식 노드에 한가지 클래스의 데이터만 존재~"에서 한가지 클래스의 데이터만 존재한다는 것이 이해가 되지 않는다.`
`한가지 클래스의 데이터만 존재: 독립변수를 기준으로 더이상 나눌 수 없는 원자성을 지닌 상태가 되었을 때는 나눠봐야 1:0으로만 나눠지기 때문에 더 이상 나누는 것이 의미없기에 더이상 나누지 않는다는 뜻 by chatGPT`

### 분류규칙을 정하는 방법
---
분류 규칙을 정하는 방법은 부모 노드와 자식 노드 간의 엔트로피를 가장 낮게 만드는 최상의 독립 변수와 기준값을 찾는 것이다. 이러한 기준을 정량화 한 것이 정보획득량(Information gain)이다. 기본적으로 모든 독립 변수와 모든 가능한 기준값에 대해 정보획득량을 구하여 가장 정보획득량이 큰 독립 변수와 기준값을 선택한다.
`엔트로피 : 정보이론이나 의사결정트리 알고리즘에서 엔트로피란 하나의 데이터 세트의 비순수성(impurity) 혹은 불명확성(Uncertainty)을 축정하는 것을 말한다.`
`의사결정트리에서 한 노드의 엔트로피는 클래스들이 얼마나 섞여있냐를 뜻한다. 낮은 엔트로피의 노드는 상대적으로 순수하고 높은 엔트로피의 노드는 상대적으로 불순하다.`
`이해: 그러니까 엔트로피는 분류가 얼마나 진행되었냐를 뜻하는 것이고 많이 불류되어 순수하면 엔트로피가 낮은 것이고 아직 적게 분류되어 여러가지로 분류가 가능하면 엔트로피가 높은 것`

### 정보획득량
---
정보획득량(Information gain)은 X라는 조건에 의해 확률 변수 Y의 엔트로피가 얼마나 감소하였는가를 나타내는 값이다. 다음처럼 Y의 엔트로피에서 X에 대한 Y의 조건부 엔트로피를 뺀 값으로 정의된다.
> IG\[Y, X\] = H\[Y\] - H\[Y|X\]

`이후 수식이 있지만 이해할 순 없었고 다만 예제를 보면 분류를 했을 때 한쪽으로 쏠림 없이 분배가 되는 것이 정보획득량이 좋은 것으로 볼 수 있었다.`
```
A 방법과 B 방법 모두 노드 분리 전에는 Y=0 인 데이터의 수와 Y=1 인 데이터의 수가 모두 40개였다.

A 방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생긴다.

- 자식 노드 A1은 Y=0 인 데이터가 30개, Y=1 인 데이터가 10개
- 자식 노드 A2은 Y=0 인 데이터가 10개, Y=1 인 데이터가 30개

B 방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생긴다.

- 자식 노드 B1은 Y=0 인 데이터가 20개, Y=1 인 데이터가 40개
- 자식 노드 B2은 Y=0 인 데이터가 20개, Y=1 인 데이터가 0개

B의 방법이 더 나은 방법
```

### Scikit-Learn의 의사결정나무 클래스
---
Scikit-Learn에서 의사결정나무는 DecisionTreeClassifier 클래스로 구현되어 있다. 여기에서는 붓꽃 분류 문제를 예를 들어 의사결정나무를 설명한다. 이 예제에서는 독립변수 공간을 공간상에 표시하기 위해 꽃의 길이와 폭만을 독립변수로 사용하였다.
`2차원 배열이어야 xy좌표를 찍기 편해서 2개만 사용했다는 뜻`
```python
from sklearn.datasets import load_iris

data= load_iris()
y = data.target
X = data.data[:,2:]
feature_names= data.feature_names[2:]

from sklearn.tree import DecisionTreeClassifier
tree1 = DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=0).fit(X,y)
```
`criterion : 판단을 내리거나 평가하는 원칙 DecisionTreeClassifier에서는 gini와 entropy가 있으나 어떤 것이 더 낫다고 말할 수 없기 때문에 데이터에 따라서 바꿔사용해도 된다.`

#python 
#machin-learning
#decisiontree
